# False discovery rate (FDR) correction
aka multiple comparison correction
aka q-values

## Intro

- a p-value of 0.05 means that there is a 5% chance that your result is random and does not accurately represent the true population underlying your statistical sample
--- aka a 5% chance that the result is a false positive aka you say it's significant when it's not
- when you run many tests, you reach this level of chance
--- for example, if you run 100 tests, you expect 5 tests with p < 0.05 that result from random chance
--- if you get 15 significant results, you have no way of knowing which are real and which are false positives
--- so you FDR correct to reduce the false positive rate (aka Type I error) relative to the number of tests being run
--- thus, the FDR value better estimates real from false positives

## Methods

- Simple but harsh: Bonferroni multiples the p-value by the number of tests being run
- Complex but more forgiving: Benjamini-Hochberg 
--- 1. ranks p-values from smallest to largest
--- 2. calculates a critical value based on each p-value's rank, total tests, and specified FDR cutoff
--- 3. compare actual p-values to critical value. all p-values ranked below the highest rank where the critical value < p-value are considered significant at the specified FDR cutoff
--- so common that it is often just called "fdr"
--- this is preferred for analyses with 100s+ of comparisons

## What to include in the correction
### Within 1  model?

y ~ A + B

- if A and B are independent, FDR correct across all A p-values separately from all B p-values
- if A and B are dependent, FDR correct across all A and B p-values together
--- and consider if A and B should be in the same model if they are dependent

- so including dependent co-variates potentially "hurts" the FDR correction of your variable(s) of interest
--- but if the model is a better fit or if those co-variates are standard in the field, it's a necessary trade-off

- Our consensus: Include all relevant co-variates, FDR correct across all variables that are inter-dependent, and then change your FDR threshold after as needed
--- honesty of reported FDR cutoff vs "hidden" if only correcting across variables of interest and reporting lower FDR cutoff

### Across multiple models?

y ~ A
y ~ A * B

- If running multiple models to pick the better one, correct within each model and keep the "best" one
- If a nested approach (e.g. take significant genes from variable A in model1 and run only those genes from variable A*B in model2)
--- Do you FDR correct across all A + A*B? More strict than 2 separate models. Defeats the purpose of the nest
--- If A and B are independent, FDR correct in each variable subset (A only, A in model with B, B, A:B)

## Multiple hypothesis testing
### Specified contrasts within lm

- as with suggestion within 1 model, consider if contrasts are dependent
--- If 3 levels (A, B, C) and want to only look at A-B and A-C, then these are not independent of A and should be corrected across p-values from both contrasts
--- If 4 levels (A, B, C, D) and want to only A-B and C-D, then these may be independent (depending on what the groups are) and you might be able to correct within each contrast separately


### Post-hoc test for 3+ levels

- Do you correct across all pairwise p-values within just 1 gene of interest? Or across all pairwise in all genes? Again, consider independent of variables and models

***