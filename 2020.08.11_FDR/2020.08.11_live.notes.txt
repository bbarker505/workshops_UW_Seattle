# False discovery rate (FDR) correction
aka multiple comparison correction
aka q-values

## Intro

- a p-value of 0.05 means that there is a 5% chance that your result is random and does not accurately represent the true population underlying your statistical sample
--- aka a 5% chance that the result is a false positive aka you say it's significant when it's not
- when you run many tests, you reach this level of chance
--- for example, if you run 100 tests, you expect 5 tests with p < 0.05 that result from random chance
--- if you get 15 significant results, you have no way of knowing which are real and which are false positives
--- so you FDR correct to reduce the false positive rate (aka Type I error) relative to the number of tests being run
--- thus, the FDR value better estimates real from false positives

## Methods

Simple but harsh:
- Bonferroni multiples the p-value by the number of tests being run

Complex but more forgiving
- Benjamini-Hochberg 
--- uses rank relative to total tests and a pre-defined significance cutoff
--- assumes that tests are INDEPENDENT
--- so common that it is often just called "fdr"
- Benjamini-Yekutieli 
--- similar to BH but assumes tests are DEPENDENT

## What to include in the correction
### For 1 model, 1 outcome

y ~ A + B

- No FDR correction necessary since you are running 1 test

### For 1 model, multiple outcomes

y1 ~ A + B
y2 ~ A + B

- If y1 and y2 are independent, then you don't need to FDR correct as there are no dependent multiple comparisons
--- In a concrete example, say you're testing how age (A) and sex (B) impact height (y1) and hair length (y2). Since height is not related to hair length, these are separate tests entirely, hence no FDR issues
--- Sometimes people will FDR correct in this case anyway because they are not 100% confident that the y-variables are fully independent. In this case, see below

- If y1 and y2 are dependent (like RNA-seq)
- AND A and B are dependent, correct using BY across all A and B p-values
- AND A and B are independent, correct using BH across A and B p-values separately

### Multiple models, 1 outcome

y ~ A
y ~ B

- If A and B are dependent, correct with BY across the 2 p-values
- If A and B are independent, correct with BH across the 2 p-values

y ~ A1 + A2
y ~ B1 + B2

- If any A variable is dependent to any B variable OR any variables within the same model are dependent, correct with BY across 4 p-values
- If As are independent from Bs AND variables within the same model are independent, correct with BH across 4 p-values

### Nested models, 1 outcome

y ~ A
y ~ A*B
y ~ A+B

- determine which model is the best fit, and use that as your 1 model

### Nested models, multiple outcomes

y1 ~ A
y2 ~ A
...
Only use y significant for A in next model
y1 ~ A*B
y2 ~ A*B

Concrete example: genes significant for media vs TB then tested for TB*RSTR interaction

- Our consensus: Following the above rules, FDR correct within the A alone model separately from the interaction model
- Consider not FDR correcting the A alone model to ensure all possible hits make it into the interaction model?

## Multiple hypothesis testing
### Specified contrasts within lm

- as with suggestion within 1 model, consider if contrasts are dependent
--- If 3 levels (A, B, C) and want to only look at A-B and A-C, then these are not independent of A and should be corrected across p-values from both contrasts
--- If 4 levels (A, B, C, D) and want to only A-B and C-D, then these may be independent (depending on what the groups are) and you might be able to correct within each contrast separately


### Post-hoc test for 3+ levels

- Do you correct across all pairwise p-values within just 1 gene of interest? Or across all pairwise in all genes? Again, consider independent of variables and models

***